{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MCP ì„œë²„ ì—°ê²° (sse)\n",
    "- MCP ì„œë²„ ìˆ˜í–‰ í›„ ì—°ê²°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[StructuredTool(name='get_current_time', description='\\n    Get current time for the specified timezone.\\n    \\n    This function returns the current time for the specified timezone.\\n    If no timezone is specified, the default timezone is \"Asia/Seoul\".\\n\\n    Args:\\n        timezone (str): The timezone to get the current time for. Default is \"Asia/Seoul\".\\n\\n    Returns:\\n        str: A string containing the current time for the specified timezone.\\n    ', args_schema={'properties': {'timezone': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': 'Asia/Seoul', 'title': 'Timezone'}}, 'title': 'get_current_timeArguments', 'type': 'object'}, response_format='content_and_artifact', coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x16a71f2e0>)]\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36magent\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mtools\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Current time in Asia/Seoul: 2025-05-08 15:39:41\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36magent\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "í˜„ì¬ ì„œìš¸ ì‹œê°„ì€ 2025ë…„ 5ì›” 8ì¼ ì˜¤í›„ 3ì‹œ 39ë¶„ 41ì´ˆì…ë‹ˆë‹¤.{'node': 'agent', 'content': AIMessageChunk(content='', additional_kwargs={}, response_metadata={'stop_reason': 'end_turn', 'stop_sequence': None}, id='run--24dd3f39-7f38-4a4d-b22c-fc7a5f2e207c', usage_metadata={'input_tokens': 537, 'output_tokens': 39, 'total_tokens': 576, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}}), 'metadata': {'langgraph_step': 3, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:db9fcddc-c1b6-a242-0573-ce17253c022a', 'checkpoint_ns': 'agent:db9fcddc-c1b6-a242-0573-ce17253c022a', 'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-haiku-20240307', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 4096}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_teddynote.messages import ainvoke_graph, astream_graph\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "model = ChatAnthropic(\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    max_tokens=4096,\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "async with MultiServerMCPClient(\n",
    "    {\n",
    "        \"weather\": {\n",
    "            \"url\": \"http://localhost:8005/sse\",\n",
    "            \"transport\": \"sse\",\n",
    "        }\n",
    "    }\n",
    ") as client:\n",
    "    print(client.get_tools())\n",
    "    agent = create_react_agent(model, client.get_tools())\n",
    "    answer = await astream_graph(agent, {\"messages\": \"ì˜¤ëŠ˜ ì„œìš¸ ì‹œê°„ì„ ì•Œë ¤ì¤˜\"})\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MCP ì„œë²„ ì—°ê²° (stdio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[StructuredTool(name='get_weather', description='\\n    Get current weather information for the specified location.\\n\\n    This function simulates a weather service by returning a fixed response.\\n    In a production environment, this would connect to a real weather API.\\n\\n    Args:\\n        location (str): The name of the location (city, region, etc.) to get weather for\\n\\n    Returns:\\n        str: A string containing the weather information for the specified location\\n    ', args_schema={'properties': {'location': {'title': 'Location', 'type': 'string'}}, 'required': ['location'], 'title': 'get_weatherArguments', 'type': 'object'}, response_format='content_and_artifact', coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x16a71f060>)]\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36magent\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mtools\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "The weather in ì„œìš¸ is sunny.\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36magent\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "ì„œìš¸ì˜ ë‚ ì”¨ëŠ” ë§‘ê³  í™”ì°½í•œ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ì˜¤ëŠ˜ì€ ì•¼ì™¸ í™œë™í•˜ê¸° ì¢‹ì€ ë‚ ì”¨ì¸ ê²ƒ ê°™ë„¤ìš”."
     ]
    }
   ],
   "source": [
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_mcp_adapters.tools import load_mcp_tools\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "# Anthropic ëª¨ë¸ ìƒì„±\n",
    "model = ChatAnthropic(\n",
    "    model_name=\"claude-3-haiku-20240307\",\n",
    "    max_tokens=4096,\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# StdIO ì„œë²„ íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "# - command: Python ì¸í„°í”„ë¦¬í„° ê²½ë¡œ\n",
    "# - args: ì‹¤í–‰í•  MCP ì„œë²„ ìŠ¤í¬ë¦½íŠ¸ ê²½ë¡œ\n",
    "server_params = StdioServerParameters(\n",
    "    command=\"./.venv/bin/python\",\n",
    "    args=[\"01_mcp_server_local.py\"],\n",
    ")\n",
    "\n",
    "# StdIO í´ë¼ì´ì–¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„œë²„ì™€ í†µì‹ \n",
    "async with stdio_client(server_params) as (read, write):\n",
    "    # í´ë¼ì´ì–¸íŠ¸ ì„¸ì…˜ ìƒì„±\n",
    "    async with ClientSession(read, write) as session:\n",
    "        # ì—°ê²° ì´ˆê¸°í™”\n",
    "        await session.initialize()\n",
    "        \n",
    "        # ë„êµ¬ ë¡œë“œ\n",
    "        tools = await load_mcp_tools(session)\n",
    "        print(tools)\n",
    "        \n",
    "        # ì—ì´ì „íŠ¸ ìƒì„±\n",
    "        agent = create_react_agent(model, tools)\n",
    "        \n",
    "        # ë„êµ¬ í˜¸ì¶œ\n",
    "        await astream_graph(agent, {\"messages\": \"ì˜¤ëŠ˜ ì„œìš¸ì˜ ë‚ ì”¨ëŠ” ì–´ë•Œ?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
